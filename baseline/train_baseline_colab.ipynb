{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ãƒã‚¤ã‚ºæ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ï¼ˆGoogle Colabç‰ˆï¼‰\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€10ã‚¯ãƒ©ã‚¹åˆ†é¡ã«ã‚ˆã‚‹ãƒã‚¤ã‚ºæ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’è¡Œã„ã¾ã™ã€‚\n",
    "\n",
    "## ğŸ“‹ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †\n",
    "\n",
    "1. **GPUã‚’æœ‰åŠ¹åŒ–**: ãƒ©ãƒ³ã‚¿ã‚¤ãƒ  â†’ ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®ã‚¿ã‚¤ãƒ—ã‚’å¤‰æ›´ â†’ GPUï¼ˆT4ï¼‰ã‚’é¸æŠ\n",
    "2. **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”Ÿæˆï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã§å®Ÿè¡Œï¼‰**:\n",
    "   - `prepare_baseline_dataset.py` ã‚’å®Ÿè¡Œã—ã¦ `baseline_dataset.pickle` ã‚’ç”Ÿæˆ\n",
    "   - ãƒã‚¤ã‚ºã‚¿ã‚¤ãƒ—: `frequency_band`ï¼ˆå‘¨æ³¢æ•°å¸¯åŸŸé›†ä¸­ãƒã‚¤ã‚ºï¼‰\n",
    "3. **ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆæ¨å¥¨: ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—ï¼‰**: \n",
    "   - å·¦å´ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ–ãƒ©ã‚¦ã‚¶ï¼ˆğŸ“ã‚¢ã‚¤ã‚³ãƒ³ï¼‰ã‚’é–‹ã\n",
    "   - `/content/` ãƒ•ã‚©ãƒ«ãƒ€ã«ä»¥ä¸‹ã®3ãƒ•ã‚¡ã‚¤ãƒ«ã‚’**ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—**:\n",
    "     - `baseline_dataset.pickle`\n",
    "     - `baseline_model.py`\n",
    "     - `evaluate_noise_detection.py`\n",
    "   - âš¡ **ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—ã®æ–¹ãŒé€Ÿã„ã§ã™ï¼**\n",
    "4. å„ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
    "\n",
    "## ğŸ“ ãƒã‚¤ã‚ºã‚¿ã‚¤ãƒ—ã«ã¤ã„ã¦\n",
    "\n",
    "ç¾åœ¨ä½¿ç”¨ã—ã¦ã„ã‚‹ãƒã‚¤ã‚ºã‚¿ã‚¤ãƒ—:\n",
    "- **`frequency_band`**: å‘¨æ³¢æ•°å¸¯åŸŸé›†ä¸­ãƒã‚¤ã‚ºï¼ˆç‰¹å®šã®å‘¨æ³¢æ•°å¸¯åŸŸã«é›†ä¸­çš„ã«ç™ºç”Ÿï¼‰\n",
    "  - é›»æºãƒã‚¤ã‚ºã€å…±æŒ¯ã€ã‚¯ãƒ­ã‚¹ãƒˆãƒ¼ã‚¯ãªã©ã‚’æ¨¡æ“¬\n",
    "  - åŒºé–“å†…ã®ç‰¹å®šã®å‘¨æ³¢æ•°å¸¯åŸŸï¼ˆ30%ç¨‹åº¦ï¼‰ã«ã‚¬ã‚¦ã‚·ã‚¢ãƒ³åˆ†å¸ƒã§ãƒã‚¤ã‚ºãŒé›†ä¸­\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "!pip install torch torchvision scikit-learn matplotlib -q\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "import os\n",
    "import time\n",
    "\n",
    "# GPUã®ç¢ºèª\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPUå: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPUãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®šï¼ˆColabç”¨ï¼‰\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n",
    "\n",
    "ä»¥ä¸‹ã®ã‚»ãƒ«ã§ã€å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¾ã™ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n",
    "required_files = [\n",
    "    'baseline_dataset.pickle',\n",
    "    'baseline_model.py',\n",
    "    'evaluate_noise_detection.py'\n",
    "]\n",
    "\n",
    "print(\"å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèªä¸­...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "missing_files = []\n",
    "for filename in required_files:\n",
    "    if os.path.exists(filename):\n",
    "        file_size = os.path.getsize(filename) / (1024 * 1024)  # MB\n",
    "        print(f\"âœ“ {filename} ({file_size:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"âœ— {filename} - è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "        missing_files.append(filename)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if missing_files:\n",
    "    print(\"\\nâš ï¸ ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“:\")\n",
    "    for f in missing_files:\n",
    "        print(f\"  - {f}\")\n",
    "    print(\"\\nğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ–¹æ³•:\")\n",
    "    print(\"  1. å·¦å´ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ–ãƒ©ã‚¦ã‚¶ï¼ˆğŸ“ã‚¢ã‚¤ã‚³ãƒ³ï¼‰ã‚’é–‹ã\")\n",
    "    print(\"  2. /content/ ãƒ•ã‚©ãƒ«ãƒ€ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—\")\n",
    "    print(\"  3. ã¾ãŸã¯ã€ä¸‹è¨˜ã®ã‚»ãƒ«ï¼ˆä»£æ›¿æ–¹æ³•ï¼‰ã‚’å®Ÿè¡Œã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\")\n",
    "    print(\"\\nğŸ’¡ ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—ã®æ–¹ãŒé€Ÿã„ã§ã™ï¼\")\n",
    "else:\n",
    "    print(\"\\nâœ… ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒæƒã£ã¦ã„ã¾ã™ï¼æ¬¡ã®ã‚»ãƒ«ã«é€²ã‚“ã§ãã ã•ã„ã€‚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_model.py ã¨ evaluate_noise_detection.py ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "from baseline_model import SimpleCNN\n",
    "from evaluate_noise_detection import evaluate_baseline_model, plot_confusion_matrix\n",
    "\n",
    "print(\"âœ“ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹\n",
    "class PSDDataset(Dataset):\n",
    "    \"\"\"PSDãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\"\"\"\n",
    "    \n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"æ¤œè¨¼\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in dataloader:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(data)\n",
    "            \n",
    "            # NaN/Infãƒã‚§ãƒƒã‚¯\n",
    "            if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
    "                print(f\"è­¦å‘Š: ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã«NaN/InfãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ\")\n",
    "                print(f\"  NaN: {torch.isnan(outputs).sum().item()}, Inf: {torch.isinf(outputs).sum().item()}\")\n",
    "                continue\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy, all_predictions, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"1ã‚¨ãƒãƒƒã‚¯ã®å­¦ç¿’\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for data, labels in dataloader:\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # é †ä¼æ’­\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        \n",
    "        # NaN/Infãƒã‚§ãƒƒã‚¯\n",
    "        if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
    "            print(f\"è­¦å‘Š: ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã«NaN/InfãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ\")\n",
    "            print(f\"  NaN: {torch.isnan(outputs).sum().item()}, Inf: {torch.isinf(outputs).sum().item()}\")\n",
    "            continue\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # NaN/Infãƒã‚§ãƒƒã‚¯\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"è­¦å‘Š: æå¤±ã«NaN/InfãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ\")\n",
    "            continue\n",
    "        \n",
    "        # é€†ä¼æ’­\n",
    "        loss.backward()\n",
    "        \n",
    "        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå‹¾é…çˆ†ç™ºã‚’é˜²ãï¼‰\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # çµ±è¨ˆ\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å‰ã«å®šç¾©ï¼‰\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿\n",
    "print(\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "with open('baseline_dataset.pickle', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "train_data = dataset['train']['data']\n",
    "train_labels = dataset['train']['labels']\n",
    "val_data = dataset['val']['data']\n",
    "val_labels = dataset['val']['labels']\n",
    "test_data = dataset['test']['data']\n",
    "test_labels = dataset['test']['labels']\n",
    "\n",
    "print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_data):,}ã‚µãƒ³ãƒ—ãƒ«\")\n",
    "print(f\"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(val_data):,}ã‚µãƒ³ãƒ—ãƒ«\")\n",
    "print(f\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_data):,}ã‚µãƒ³ãƒ—ãƒ«\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›\n",
    "train_data = torch.FloatTensor(train_data)\n",
    "train_labels = torch.LongTensor(train_labels)\n",
    "val_data = torch.FloatTensor(val_data)\n",
    "val_labels = torch.LongTensor(val_labels)\n",
    "test_data = torch.FloatTensor(test_data)\n",
    "test_labels = torch.LongTensor(test_labels)\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–ã‚’ç„¡åŠ¹åŒ–ï¼ˆãƒã‚¤ã‚ºã‚’ä¿æŒã™ã‚‹ãŸã‚ï¼‰\n",
    "# æ­£è¦åŒ–ã«ã‚ˆã‚Šãƒã‚¤ã‚ºãŒæ¶ˆãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€ä¸€æ—¦ç„¡åŠ¹åŒ–\n",
    "print(\"\\nãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–ä¸­...\")\n",
    "# train_mean = train_data.mean()\n",
    "# train_std = train_data.std()\n",
    "# print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡: {train_mean:.6e}\")\n",
    "# print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åå·®: {train_std:.6e}\")\n",
    "\n",
    "# ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆã‚’ä½¿ç”¨ï¼‰\n",
    "# train_data = (train_data - train_mean) / (train_std + 1e-8)\n",
    "# val_data = (val_data - train_mean) / (train_std + 1e-8)\n",
    "# test_data = (test_data - train_mean) / (train_std + 1e-8)\n",
    "\n",
    "\n",
    "# ãƒ©ãƒ™ãƒ«ã®ç¢ºèªï¼ˆ10ã‚¯ãƒ©ã‚¹åˆ†é¡ç”¨ï¼‰\n",
    "print(f\"\\nãƒ©ãƒ™ãƒ«ã®ç¢ºèª:\")\n",
    "print(f\"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ãƒ©ãƒ™ãƒ«ç¯„å›²: {train_labels.min().item()} - {train_labels.max().item()}\")\n",
    "print(f\"  æœŸå¾…ã•ã‚Œã‚‹ç¯„å›²: 0 - {NUM_CLASSES-1}\")\n",
    "if train_labels.max().item() >= NUM_CLASSES:\n",
    "    raise ValueError(f\"ã‚¨ãƒ©ãƒ¼: ãƒ©ãƒ™ãƒ«ã®æœ€å¤§å€¤({train_labels.max().item()})ãŒNUM_CLASSES({NUM_CLASSES})ä»¥ä¸Šã§ã™ã€‚\\nãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’10ã‚¯ãƒ©ã‚¹ç”¨ã«å†ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\")\n",
    "print(\"âœ“ ãƒ©ãƒ™ãƒ«ãŒæ­£ã—ã„ç¯„å›²å†…ã§ã™\")\n",
    "print(\"âœ“ ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸï¼ˆãƒã‚¤ã‚ºã‚’ä¿æŒã™ã‚‹ãŸã‚ï¼‰\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ã‚°å¤‰æ›ï¼ˆã‚¹ã‚±ãƒ¼ãƒ«ã‚’é©åˆ‡ãªç¯„å›²ã«èª¿æ•´ï¼‰\n",
    "print(\"\\nãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°å¤‰æ›ä¸­...\")\n",
    "train_data = torch.log(train_data.clamp(min=1e-30))\n",
    "val_data = torch.log(val_data.clamp(min=1e-30))\n",
    "test_data = torch.log(test_data.clamp(min=1e-30))\n",
    "print(f\"ãƒ­ã‚°å¤‰æ›å¾Œã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡: {train_data.mean():.4f}\")\n",
    "print(f\"ãƒ­ã‚°å¤‰æ›å¾Œã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åå·®: {train_data.std():.4f}\")\n",
    "print(f\"ãƒ­ã‚°å¤‰æ›å¾Œã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²: [{train_data.min():.4f}, {train_data.max():.4f}]\")\n",
    "print(\"âœ“ ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ã‚°å¤‰æ›å®Œäº†\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆæ”¹å–„ç‰ˆï¼‰\n",
    "BATCH_SIZE = 64  # 32 â†’ 64ã«å¤‰æ›´ï¼ˆã‚ˆã‚Šå®‰å®šã—ãŸå­¦ç¿’ï¼‰\n",
    "LEARNING_RATE = 0.001  # éå­¦ç¿’ã‚’æŠ‘åˆ¶ã™ã‚‹ãŸã‚0.001ã«å¤‰æ›´\n",
    "NUM_EPOCHS = 50  # ã‚ˆã‚Šå¤šãã®ã‚¨ãƒãƒƒã‚¯ã§å­¦ç¿’ï¼ˆEarly Stoppingã§åˆ¶å¾¡ï¼‰\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ\n",
    "model = SimpleCNN(num_classes=NUM_CLASSES).to(device)\n",
    "print(f\"ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# DataLoaderã®ä½œæˆ\n",
    "train_dataset = PSDDataset(train_data, train_labels)\n",
    "val_dataset = PSDDataset(val_data, val_labels)\n",
    "test_dataset = PSDDataset(test_data, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ã‚¯ãƒ©ã‚¹é‡ã¿ã®è¨ˆç®—ï¼ˆã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã«å¯¾å¿œï¼‰\n",
    "# ã‚¯ãƒ©ã‚¹é‡ã¿ã®è¨ˆç®—ï¼ˆã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã«å¯¾å¿œï¼‰\n",
    "print(\"\\nã‚¯ãƒ©ã‚¹é‡ã¿ã‚’è¨ˆç®—ä¸­...\")\n",
    "print(f\"ãƒ©ãƒ™ãƒ«ã®ç¯„å›²: {train_labels.min().item()} - {train_labels.max().item()}\")\n",
    "print(f\"NUM_CLASSES: {NUM_CLASSES}\")\n",
    "# ãƒ©ãƒ™ãƒ«ãŒNUM_CLASSESã‚’è¶…ãˆã¦ã„ãªã„ã‹ç¢ºèª\n",
    "if train_labels.max().item() >= NUM_CLASSES:\n",
    "    raise ValueError(f\"ã‚¨ãƒ©ãƒ¼: ãƒ©ãƒ™ãƒ«ã®æœ€å¤§å€¤({train_labels.max().item()})ãŒNUM_CLASSES({NUM_CLASSES})ä»¥ä¸Šã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’10ã‚¯ãƒ©ã‚¹ç”¨ã«å†ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\")\n",
    "# ã‚¯ãƒ©ã‚¹é‡ã¿ã‚’è¨ˆç®—ï¼ˆç¢ºå®Ÿã«NUM_CLASSESè¦ç´ ã«ã™ã‚‹ï¼‰\n",
    "label_counts_full = torch.bincount(train_labels)\n",
    "label_counts = torch.zeros(NUM_CLASSES, dtype=torch.long)\n",
    "for i in range(min(NUM_CLASSES, len(label_counts_full))):\n",
    "    label_counts[i] = label_counts_full[i]\n",
    "# é‡ã¿ = ç·ã‚µãƒ³ãƒ—ãƒ«æ•° / (ã‚¯ãƒ©ã‚¹æ•° * å„ã‚¯ãƒ©ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ«æ•°)\n",
    "class_weights = len(train_labels) / (NUM_CLASSES * (label_counts.float() + 1e-8))\n",
    "class_weights = class_weights / class_weights.sum() * NUM_CLASSES  # æ­£è¦åŒ–\n",
    "print(f\"ã‚¯ãƒ©ã‚¹é‡ã¿ã®å½¢çŠ¶: {class_weights.shape}\")\n",
    "print(f\"ã‚¯ãƒ©ã‚¹é‡ã¿ã®ç¯„å›²: [{class_weights.min():.3f}, {class_weights.max():.3f}]\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)  # é‡ã¿æ¸›è¡°ã‚’è¿½åŠ \n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "print(\"\\nâœ“ ãƒ¢ãƒ‡ãƒ«ã¨å­¦ç¿’è¨­å®šã®æº–å‚™å®Œäº†\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’å®Ÿè¡Œï¼ˆ100ã‚¨ãƒãƒƒã‚¯ï¼‰\n",
    "# å­¦ç¿’å±¥æ­´\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "best_val_acc = 0\n",
    "best_model_state = None\n",
    "patience = 10  # Early Stoppingã®patience\n",
    "patience_counter = 0  # Early Stoppingã®ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼\n",
    "best_val_acc = 0\n",
    "\n",
    "print(\"å­¦ç¿’é–‹å§‹...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ç·ã‚¨ãƒãƒƒã‚¯æ•°: {NUM_EPOCHS}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # å­¦ç¿’\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # æ¤œè¨¼\n",
    "    val_loss, val_acc, _, _ = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # å­¦ç¿’ç‡ã®èª¿æ•´\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # å±¥æ­´ã®ä¿å­˜\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n",
    "    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¨Early Stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        patience_counter = 0  # ãƒ™ã‚¹ãƒˆç²¾åº¦æ›´æ–°æ™‚ã«ãƒªã‚»ãƒƒãƒˆ\n",
    "    else:\n",
    "        patience_counter += 1  # ãƒ™ã‚¹ãƒˆç²¾åº¦ãŒæ›´æ–°ã•ã‚Œãªã‹ã£ãŸ\n",
    "    \n",
    "    # Early Stoppingãƒã‚§ãƒƒã‚¯\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nâš  Early Stopping: {patience}ã‚¨ãƒãƒƒã‚¯é€£ç¶šã§æ”¹å–„ãªã—ã€‚å­¦ç¿’ã‚’åœæ­¢ã—ã¾ã™ã€‚\")\n",
    "        print(f\"   ãƒ™ã‚¹ãƒˆæ¤œè¨¼ç²¾åº¦: {best_val_acc:.2f}% (ã‚¨ãƒãƒƒã‚¯ {epoch+1-patience})\")\n",
    "        break\n",
    "    \n",
    "    # ãƒ­ã‚°å‡ºåŠ›ï¼ˆ5ã‚¨ãƒãƒƒã‚¯ã”ã¨ã€ã¾ãŸã¯æœ€åˆã¨æœ€å¾Œï¼‰\n",
    "    # ãƒ­ã‚°å‡ºåŠ›ï¼ˆæ¯ã‚¨ãƒãƒƒã‚¯ã€ã¾ãŸã¯æœ€åˆã¨æœ€å¾Œï¼‰\n",
    "    if (epoch + 1) % 1 == 0 or epoch == 0 or (epoch + 1) == NUM_EPOCHS:\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        elapsed_time = time.time() - start_time\n",
    "        remaining_epochs = NUM_EPOCHS - (epoch + 1)\n",
    "        estimated_remaining = (elapsed_time / (epoch + 1)) * remaining_epochs if epoch > 0 else 0\n",
    "        \n",
    "        # å­¦ç¿’ç‡ã®ç¢ºèª\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’ç¢ºèªï¼ˆæœ€åˆã®ã‚¨ãƒãƒƒã‚¯ã®ã¿ï¼‰\n",
    "        if epoch == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                sample_data = train_data[:1].to(device)\n",
    "                sample_output = model(sample_data)\n",
    "                print(f\"\\n=== ã‚¨ãƒãƒƒã‚¯ {epoch+1} ã®è©³ç´° ===\")\n",
    "                print(f\"  å­¦ç¿’ç‡: {current_lr:.6f}\")\n",
    "                print(f\"  ã‚µãƒ³ãƒ—ãƒ«å‡ºåŠ›: {sample_output[0].tolist()}\")\n",
    "                print(f\"  äºˆæ¸¬ã‚¯ãƒ©ã‚¹: {sample_output[0].argmax().item()}\")\n",
    "                print(f\"  å‡ºåŠ›ã®ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹: {torch.softmax(sample_output[0], dim=0).tolist()}\")\n",
    "            model.train()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] ({epoch_time:.1f}ç§’)\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"  å­¦ç¿’ç‡: {current_lr:.6f}\")\n",
    "        if epoch > 0:\n",
    "            val_loss_change = val_losses[-1] - val_losses[-2] if len(val_losses) > 1 else 0\n",
    "            val_acc_change = val_accuracies[-1] - val_accuracies[-2] if len(val_accuracies) > 1 else 0\n",
    "            print(f\"  æ¤œè¨¼æå¤±ã®å¤‰åŒ–: {val_loss_change:+.4f}, æ¤œè¨¼ç²¾åº¦ã®å¤‰åŒ–: {val_acc_change:+.2f}%\")\n",
    "        if estimated_remaining > 0:\n",
    "            print(f\"  çµŒéæ™‚é–“: {elapsed_time/60:.1f}åˆ†, æ®‹ã‚Šè¦‹ç©ã‚‚ã‚Š: {estimated_remaining/60:.1f}åˆ†\")\n",
    "        print()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬çµæœã‚’å–å¾—\n",
    "print(\"\\nãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬çµæœã‚’å–å¾—ä¸­...\")\n",
    "model.eval()\n",
    "all_test_predictions = []\n",
    "all_test_labels = []\n",
    "all_test_outputs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        all_test_predictions.extend(predicted.cpu().numpy())\n",
    "        all_test_labels.extend(labels.cpu().numpy())\n",
    "        all_test_outputs.extend(torch.softmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "# äºˆæ¸¬çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"true_label\": all_test_labels,\n",
    "    \"predicted_label\": all_test_predictions,\n",
    "    \"correct\": [int(t == p) for t, p in zip(all_test_labels, all_test_predictions)]\n",
    "})\n",
    "\n",
    "# å„ã‚¯ãƒ©ã‚¹ã®ç¢ºç‡ã‚‚è¿½åŠ \n",
    "for i in range(10):\n",
    "    results_df[f\"prob_class_{i}\"] = [probs[i] for probs in all_test_outputs]\n",
    "\n",
    "results_df.to_csv(\"prediction_results.csv\", index=False)\n",
    "print(\"äºˆæ¸¬çµæœã‚’ prediction_results.csv ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n",
    "\n",
    "# æ··åŒè¡Œåˆ—ã‚‚ä¿å­˜\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(all_test_labels, all_test_predictions)\n",
    "cm_df = pd.DataFrame(cm, index=[f\"True_{i}\" for i in range(10)], columns=[f\"Pred_{i}\" for i in range(10)])\n",
    "cm_df.to_csv(\"confusion_matrix.csv\")\n",
    "print(\"æ··åŒè¡Œåˆ—ã‚’ confusion_matrix.csv ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n",
    "\n",
    "# çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º\n",
    "print(\"\\n=== äºˆæ¸¬çµæœã®çµ±è¨ˆ ===\")\n",
    "print(f\"æ­£è§£æ•°: {results_df['correct'].sum()} / {len(results_df)}\")\n",
    "print(f\"ç²¾åº¦: {results_df['correct'].mean()*100:.2f}%\")\n",
    "print(\"\\nã‚¯ãƒ©ã‚¹ã”ã¨ã®æ­£è§£æ•°:\")\n",
    "for i in range(10):\n",
    "    class_mask = results_df[\"true_label\"] == i\n",
    "    correct_count = results_df[class_mask][\"correct\"].sum()\n",
    "    total_count = class_mask.sum()\n",
    "    print(f\"  ã‚¯ãƒ©ã‚¹ {i}: {correct_count} / {total_count} ({correct_count/total_count*100:.1f}%)\")\n",
    "print(f\"å­¦ç¿’å®Œäº†ï¼ç·æ™‚é–“: {total_time/60:.1f}åˆ† ({total_time/3600:.2f}æ™‚é–“)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ†ã‚¹ãƒˆè©•ä¾¡\n",
    "model.load_state_dict(best_model_state)\n",
    "print(f\"ãƒ™ã‚¹ãƒˆæ¤œè¨¼ç²¾åº¦: {best_val_acc:.2f}%\")\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡\n",
    "print(\"\\nãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ä¸­...\")\n",
    "test_loss, test_acc, test_predictions, test_labels_list = validate(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(f\"ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_acc:.2f}%\")\n",
    "\n",
    "# è©³ç´°ãªè©•ä¾¡\n",
    "results = evaluate_baseline_model(test_predictions, test_labels_list)\n",
    "print(f\"\\nè©³ç´°ãªè©•ä¾¡çµæœ:\")\n",
    "print(f\"  Accuracy: {results['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {results['precision']:.4f}\")\n",
    "print(f\"  Recall: {results['recall']:.4f}\")\n",
    "print(f\"  F1-score: {results['f1_score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ··åŒè¡Œåˆ—ã‚’å¯è¦–åŒ–\n",
    "plot_confusion_matrix(\n",
    "    results['confusion_matrix'],\n",
    "    title='æ··åŒè¡Œåˆ—ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰',\n",
    "    save_path='baseline_confusion_matrix.png'\n",
    ")\n",
    "print(\"æ··åŒè¡Œåˆ—ã‚’ 'baseline_confusion_matrix.png' ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n",
    "\n",
    "# ç”»åƒã‚’è¡¨ç¤º\n",
    "from IPython.display import Image, display\n",
    "display(Image('baseline_confusion_matrix.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’æ›²ç·šã‚’å¯è¦–åŒ–\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(train_losses, label='è¨“ç·´æå¤±', color='blue')\n",
    "axes[0].plot(val_losses, label='æ¤œè¨¼æå¤±', color='red')\n",
    "axes[0].set_xlabel('ã‚¨ãƒãƒƒã‚¯')\n",
    "axes[0].set_ylabel('æå¤±')\n",
    "axes[0].set_title('å­¦ç¿’æ›²ç·šï¼ˆæå¤±ï¼‰')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(train_accuracies, label='è¨“ç·´ç²¾åº¦', color='blue')\n",
    "axes[1].plot(val_accuracies, label='æ¤œè¨¼ç²¾åº¦', color='red')\n",
    "axes[1].set_xlabel('ã‚¨ãƒãƒƒã‚¯')\n",
    "axes[1].set_ylabel('ç²¾åº¦ (%)')\n",
    "axes[1].set_title('å­¦ç¿’æ›²ç·šï¼ˆç²¾åº¦ï¼‰')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('baseline_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "print(\"å­¦ç¿’æ›²ç·šã‚’ 'baseline_training_curves.png' ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'test_accuracy': test_acc,\n",
    "    'results': results\n",
    "}, 'baseline_model.pth')\n",
    "print(\"ãƒ¢ãƒ‡ãƒ«ã‚’ 'baseline_model.pth' ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "\n",
    "ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã€çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ï¼š\n",
    "- `baseline_model.pth` (å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«)\n",
    "- `baseline_confusion_matrix.png` (æ··åŒè¡Œåˆ—)\n",
    "- `baseline_training_curves.png` (å­¦ç¿’æ›²ç·š)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "from google.colab import files\n",
    "\n",
    "files.download('baseline_model.pth')\n",
    "files.download('baseline_confusion_matrix.png')\n",
    "files.download('baseline_training_curves.png')\n",
    "print(\"âœ“ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä»£æ›¿æ–¹æ³•: ã‚³ãƒ¼ãƒ‰ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—ãŒä½¿ãˆãªã„å ´åˆï¼‰\n",
    "\n",
    "ä¸Šè¨˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªã§ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ã¿ã€ã“ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”¨ï¼ˆä»£æ›¿æ–¹æ³•ï¼‰\n",
    "# âš ï¸ ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—ã®æ–¹ãŒé€Ÿã„ã®ã§ã€ãã¡ã‚‰ã‚’æ¨å¥¨ã—ã¾ã™\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "print(\"ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„:\")\n",
    "print(\"1. baseline_dataset.pickle\")\n",
    "print(\"2. baseline_model.py\")\n",
    "print(\"3. evaluate_noise_detection.py\")\n",
    "print(\"\\nã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "print(\"\\nã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼\")\n",
    "for filename in uploaded.keys():\n",
    "    print(f\"  âœ“ {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_model.py ã¨ evaluate_noise_detection.py ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "from baseline_model import SimpleCNN\n",
    "from evaluate_noise_detection import evaluate_baseline_model, plot_confusion_matrix\n",
    "\n",
    "print(\"âœ“ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10  # Google Colabã®GPUã§é«˜é€Ÿå®Ÿè¡Œ\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# DataLoaderã®ä½œæˆ\n",
    "train_dataset = PSDDataset(train_data, train_labels)\n",
    "val_dataset = PSDDataset(val_data, val_labels)\n",
    "test_dataset = PSDDataset(test_data, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ\n",
    "model = SimpleCNN(num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã‚’ç¢ºèª\n",
    "print(\"\\n=== ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ç¢ºèª ===\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ\n",
    "    dummy_input = torch.randn(1, 3000).to(device)\n",
    "    dummy_output = model(dummy_input)\n",
    "    print(f\"  ãƒ€ãƒŸãƒ¼å…¥åŠ›ã®å½¢çŠ¶: {dummy_input.shape}\")\n",
    "    print(f\"  ãƒ€ãƒŸãƒ¼å‡ºåŠ›ã®å½¢çŠ¶: {dummy_output.shape}\")\n",
    "    print(f\"  ãƒ€ãƒŸãƒ¼å‡ºåŠ›ã®ç¯„å›²: [{dummy_output.min().item():.4f}, {dummy_output.max().item():.4f}]\")\n",
    "    print(f\"  ãƒ€ãƒŸãƒ¼å‡ºåŠ›ã®å¹³å‡: {dummy_output.mean().item():.4f}\")\n",
    "    print(f\"  ãƒ€ãƒŸãƒ¼å‡ºåŠ›ã®æ¨™æº–åå·®: {dummy_output.std().item():.4f}\")\n",
    "    print(f\"  å„ã‚¯ãƒ©ã‚¹ã®å‡ºåŠ›: {dummy_output[0].tolist()}\")\n",
    "    print(f\"  ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹å¾Œã®ç¢ºç‡: {torch.softmax(dummy_output[0], dim=0).tolist()}\")\n",
    "model.train()\n",
    "print(f\"ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# æå¤±é–¢æ•°ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’å®Ÿè¡Œ\n",
    "import time\n",
    "\n",
    "# å­¦ç¿’å±¥æ­´\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "best_val_acc = 0\n",
    "best_model_state = None\n",
    "\n",
    "print(\"å­¦ç¿’é–‹å§‹...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ç·ã‚¨ãƒãƒƒã‚¯æ•°: {NUM_EPOCHS}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # å­¦ç¿’\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # æ¤œè¨¼\n",
    "    val_loss, val_acc, _, _ = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # å­¦ç¿’ç‡ã®èª¿æ•´\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # å±¥æ­´ã®ä¿å­˜\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        patience_counter = 0  # ãƒ™ã‚¹ãƒˆç²¾åº¦æ›´æ–°æ™‚ã«ãƒªã‚»ãƒƒãƒˆ\n",
    "    else:\n",
    "        patience_counter += 1  # ãƒ™ã‚¹ãƒˆç²¾åº¦ãŒæ›´æ–°ã•ã‚Œãªã‹ã£ãŸ\n",
    "    \n",
    "    # Early Stoppingãƒã‚§ãƒƒã‚¯\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nâš  Early Stopping: {patience}ã‚¨ãƒãƒƒã‚¯é€£ç¶šã§æ”¹å–„ãªã—ã€‚å­¦ç¿’ã‚’åœæ­¢ã—ã¾ã™ã€‚\")\n",
    "        print(f\"   ãƒ™ã‚¹ãƒˆæ¤œè¨¼ç²¾åº¦: {best_val_acc:.2f}% (ã‚¨ãƒãƒƒã‚¯ {epoch+1-patience})\")\n",
    "        break\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    # ãƒ­ã‚°å‡ºåŠ›ï¼ˆ5ã‚¨ãƒãƒƒã‚¯ã”ã¨ã€ã¾ãŸã¯æœ€åˆã¨æœ€å¾Œï¼‰\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0 or (epoch + 1) == NUM_EPOCHS:\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        elapsed_time = time.time() - start_time\n",
    "        remaining_epochs = NUM_EPOCHS - (epoch + 1)\n",
    "        estimated_remaining = (elapsed_time / (epoch + 1)) * remaining_epochs\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] ({epoch_time:.1f}ç§’)\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"  çµŒéæ™‚é–“: {elapsed_time/60:.1f}åˆ†, æ®‹ã‚Šè¦‹ç©ã‚‚ã‚Š: {estimated_remaining/60:.1f}åˆ†\")\n",
    "        print()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"å­¦ç¿’å®Œäº†ï¼ç·æ™‚é–“: {total_time/60:.1f}åˆ† ({total_time/3600:.2f}æ™‚é–“)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
